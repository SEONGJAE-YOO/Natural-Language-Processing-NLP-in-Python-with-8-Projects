{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNcphyUxXvow"
   },
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JC6TSV87hS8Y"
   },
   "source": [
    "- 어간추출 (stemming)\n",
    "\n",
    "어간(Stem)을 추출하는 작업을 어간 추출(stemming)이라고 합니다. 어간 추출은 형태학적 분석을 단순화한 버전이라고 볼 수도 있고, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고 볼 수도 있습니다. 다시 말해, 이 작업은 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있습니다.\n",
    "\n",
    "어간 추출(Stemming)\n",
    "어간 추출(Stemming)이란 단어로 부터 어간(Stem)을 추출하는 작업을 뜻합니다. 어간 추출은 뒤이어 설명할 표제어 추출보다 성능이 떨어집니다. 즉, 일부 철자가 훼손된 어근 단어를 추출합니다. 단어를 보고 어림짐작하여 어미를 잘라 어간을 추출하기 때문입니다. NLTK의 어간 추출 Stemmer로는 대표적으로 Porter, Lancaster, Snowball Stemmer가 있습니다. Porter Stemmer와 Lancaster Stemmer를 비교해보겠습니다. 더자세한 내용은 밑 링크 참고!\n",
    "\n",
    "https://bkshin.tistory.com/entry/NLP-4-%EC%96%B4%EA%B0%84-%EC%B6%94%EC%B6%9CStemming%EA%B3%BC-%ED%91%9C%EC%A0%9C%EC%96%B4-%EC%B6%94%EC%B6%9CLemmatization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3IuWkK0hTYv"
   },
   "source": [
    "Playing \n",
    "\n",
    "Play   \n",
    "\n",
    "Played\n",
    "\n",
    "=> Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HTx_Asm6XwsV"
   },
   "outputs": [],
   "source": [
    "words = ['run','runner','running','ran','runs','easily','fairly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3uQQZyfRh5cG"
   },
   "outputs": [],
   "source": [
    "import nltk #NLTK는 Porter, Lancaster, Regexp, Snowball Stemmer 클래스를 제공한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8wpXkqgFh9le"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import  SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WaiuXm4NiOw1"
   },
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "mjAS8rxtieiW",
    "outputId": "4d832ecf-3c81-4193-f130-98eb79f877de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ------  run\n",
      "runner ------  runner\n",
      "running ------  run\n",
      "ran ------  ran\n",
      "runs ------  run\n",
      "easily ------  easili\n",
      "fairly ------  fairli\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "  print(word + ' ------  ' +p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "t7nMUaiSi2AO",
    "outputId": "1804c577-8126-47cf-dbbd-70c9bc42b98c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ------  run\n",
      "runner ------  runner\n",
      "running ------  run\n",
      "ran ------  ran\n",
      "runs ------  run\n",
      "easily ------  easili\n",
      "fairly ------  fair\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "  print(word + ' ------  ' +s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
     ]
    }
   ],
   "source": [
    "# 예제2\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "s = PorterStemmer()\n",
    "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thi',\n",
       " 'wa',\n",
       " 'not',\n",
       " 'the',\n",
       " 'map',\n",
       " 'we',\n",
       " 'found',\n",
       " 'in',\n",
       " 'billi',\n",
       " 'bone',\n",
       " \"'s\",\n",
       " 'chest',\n",
       " ',',\n",
       " 'but',\n",
       " 'an',\n",
       " 'accur',\n",
       " 'copi',\n",
       " ',',\n",
       " 'complet',\n",
       " 'in',\n",
       " 'all',\n",
       " 'thing',\n",
       " '--',\n",
       " 'name',\n",
       " 'and',\n",
       " 'height',\n",
       " 'and',\n",
       " 'sound',\n",
       " '--',\n",
       " 'with',\n",
       " 'the',\n",
       " 'singl',\n",
       " 'except',\n",
       " 'of',\n",
       " 'the',\n",
       " 'red',\n",
       " 'cross',\n",
       " 'and',\n",
       " 'the',\n",
       " 'written',\n",
       " 'note',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s.stem(w) for w in words] #어간 추출하기 전에 word_tokenize로 텍스트 분리하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGBsewbcjKF3"
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 표제어 추출\n",
    "\n",
    "일반적으로 어간 추출보다 표제어 추출이 더 정확히 어근 단어를 찾아줍니다.\n",
    "\n",
    "표제어 추출은 WordNetLemmatizer를 주로 사용합니다.\n",
    "\n",
    "표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행하는 것이다. 형태소란 ‘의미를 가진 가장 작은 단위’를 뜻한다. 그리고 형태학(Morphology)이란, 형태소로부터 단어들을 만들어가는 학문을 뜻한다.\n",
    "형태소는 두 가지 종류가 있다. 각각 stem(어간)과 affix(접사)이다.\n",
    "stem(어간) : 단어의 의미를 담고 있는 단어의 핵심 부분.\n",
    "affix(접사) : 단어에 추가적인 의미를 주는 부분."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uHIVmMBSjNCo"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rVdCgJRtjQoW"
   },
   "outputs": [],
   "source": [
    "doc1 = nlp(\"The striped bats are hanging on their feet for best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "6h7EcdHSjkMX",
    "outputId": "32e05c0b-d04f-45bd-a06a-af44b806d8c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \t the\n",
      "striped \t striped\n",
      "bats \t bat\n",
      "are \t be\n",
      "hanging \t hang\n",
      "on \t on\n",
      "their \t their\n",
      "feet \t foot\n",
      "for \t for\n",
      "best \t good\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "  print(token.text, '\\t', token.lemma_)\n",
    "#표제어 추출은 단어들이 서로 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지를 판단한다. 예를 들어 am,are,is는 서로 다른 스펠링이지만, 그 뿌리 단어는 be이기 때문에 이 단어들의 Lemma는 be라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "1j5yTHz0kDIz",
    "outputId": "a426aae5-142d-442d-a73a-6129ba60c210"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ------  the\n",
      "striped ------  stripe\n",
      "bats ------  bat\n",
      "are ------  are\n",
      "hanging ------  hang\n",
      "on ------  on\n",
      "their ------  their\n",
      "feet ------  feet\n",
      "for ------  for\n",
      "best ------  best\n"
     ]
    }
   ],
   "source": [
    "s1 = \"The striped bats are hanging on their feet for best\"\n",
    "for word in s1.split():\n",
    "  print(word + ' ------  ' +p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형태학적 파싱은 두 가지 구성 요소를 분리하는 작업을 뜻한다. 가령, cats라는 단어에 대해 형태학적 파싱을 수행한다면, 형태학적 파싱은 결과로 cat(어간)와 -s(접사)를 분리한다. 물론, 꼭 두 가지로 분리되지 않는 경우도 있다. 가령, 단어 fox는 형태학적 파싱을 한다고 하더라도 더 이상 분리를 할 수 없다. fox는 독립적인 형태소이기 때문이다.\n",
    "NLTK에서는 표제어 추출을 위한 도구인 WordNetLemmatizer를 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['policy',\n",
       " 'doing',\n",
       " 'organization',\n",
       " 'have',\n",
       " 'going',\n",
       " 'love',\n",
       " 'life',\n",
       " 'fly',\n",
       " 'dy',\n",
       " 'watched',\n",
       " 'ha',\n",
       " 'starting']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "n = WordNetLemmatizer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives','fly', 'dies', 'watched', 'has', 'starting']\n",
    "[n.lemmatize(w) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표제어 추출은 어간 추출(Stemming)과는 달리(다음에서 다룸) 단어의 형태가 적절히 보존되는 양상을 볼 수있다. 하지만 그럼에도 위의 결과에서는 dy나 ha와 같이 의미를 알 수 없는 적절하지 못한 단어를 출력하고 있다. 이는 표제어 추출기(Lemmatizer)가 본래 단어의 품사 정보를 알아야만 정확한 결과를 얻을 수 있기 때문이다.\n",
    "WordNetLemmatizer는 입력으로 단어의 품사를 지정해줄 수 있다. 즉, dies와 watched, has가 문장에서 동사로 쓰였다는 것을 알려준다면 표제어 추출기는 품사의 정보를 보존하면서 정확한 Lemma를 출력하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('dies','v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wathced'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('wathced', 'v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('has', 'v')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Stemming and Lemmatization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:text_analysis]",
   "language": "python",
   "name": "conda-env-text_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
